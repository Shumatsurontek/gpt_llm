vocab_size: 8000
# Data paths
train_ids: data/train_ids.pt
val_ids: data/val_ids.pt
seq_len: 2048

# Tokenizer
tokenizer_path: artifacts/tokenizer.json

# Model (~160M params)
dim: 896
n_layers: 12
n_heads: 14
ffn_mult: 4
max_seq_len: 2048
local_window: 2048
norm: layernorm
use_rope: true
use_rel_bias: false
tie_weights: true

dropout: 0.1
attn_dropout: 0.1

# Optim
lr: 0.0003
weight_decay: 0.01
betas: [0.9, 0.95]
warmup_steps: 200
max_steps: 1000
grad_clip: 1.0
batch_size: 8

# Training
seed: 42
device: mps
ckpt_dir: artifacts/checkpoints

# Logging
wandb_enabled: false
wandb_project: llm-scratch
wandb_run_name: null
