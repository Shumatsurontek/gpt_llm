vocab_size: 8000
# Data paths
train_ids: data/train_ids.pt
val_ids: data/val_ids.pt
seq_len: 2048

# Tokenizer
tokenizer_path: artifacts/tokenizer.json

# Model (~400M params)
dim: 1024
n_layers: 24
n_heads: 16
ffn_mult: 4
max_seq_len: 2048
local_window: 1024
norm: rmsnorm
use_rope: true
use_rel_bias: false
tie_weights: true

dropout: 0.1
attn_dropout: 0.0

# Optim
lr: 0.0002
weight_decay: 0.01
betas: [0.9, 0.95]
warmup_steps: 1000
max_steps: 2000
grad_clip: 1.0
batch_size: 4

# Training
seed: 42
device: mps
ckpt_dir: artifacts/checkpoints

# Logging
wandb_enabled: false
wandb_project: llm-scratch
wandb_run_name: null
